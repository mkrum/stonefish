# Distributed training configuration
# For multi-GPU setups with DDP

training:
  # Basic training params
  epochs: 20
  log_interval: 50
  save_interval: 500
  output_dir: models/distributed_run

  # Distributed training settings
  distributed: true
  backend: nccl  # Use nccl for GPU, gloo for CPU
  init_method: env://  # Use environment variables for initialization

  # Mixed precision training
  use_amp: true
  gradient_clipping: 1.0

  # Learning rate scheduling
  lr_scheduler:
    _target_: torch.optim.lr_scheduler.CosineAnnealingLR
    T_max: 20  # Should match epochs
    eta_min: 0.00001

# Optimizer with distributed-friendly settings
optimizer:
  _target_: torch.optim.AdamW
  lr: 0.001
  weight_decay: 0.01
  betas: [0.9, 0.999]
  eps: 1e-8

# Loss function
loss:
  _target_: torch.nn.CrossEntropyLoss
  label_smoothing: 0.1

# Wandb integration for distributed training
wandb:
  project: chess-distributed
  name: distributed-resnet
  tags: [distributed, resnet, chess, ddp]
  mode: online  # Set to 'disabled' to turn off wandb

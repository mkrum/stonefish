# Example distributed training configuration for ResNet models
# Use with torchrun: torchrun --nproc_per_node=4 -m stonefish.train --config configs/train_resnet_distributed.yml log_path

model:
  _target_: stonefish.resnet.ChessResNet
  _args_:
    - cuda  # device argument
  input_dim: 69
  hidden_dim: 4096
  num_blocks: 8
  output_dim: 5700

opt:
  _target_: torch.optim.AdamW
  lr: 0.001
  weight_decay: 0.01

train_dl:
  _target_: torch.utils.data.DataLoader
  dataset:
    _target_: stonefish.dataset.HuggingFaceChessDataset
    split: train
    sample_size: 100000
    dataset_name: mkrum/ParsedChess
  batch_size: 256
  shuffle: true
  num_workers: 4
  pin_memory: true
  drop_last: true

test_dl:
  _target_: torch.utils.data.DataLoader
  dataset:
    _target_: stonefish.dataset.HuggingFaceChessDataset
    split: test
    sample_size: 10000
    dataset_name: mkrum/ParsedChess
  batch_size: 256
  shuffle: false
  num_workers: 2
  pin_memory: true
  drop_last: false

# Use the distributed context
pretrain_context:
  _target_: stonefish.train.DistributedPreTrainContext
  train_fn:
    _target_: stonefish.train.train_step
  eval_fn:
    _target_: stonefish.eval.base.eval_model
  train_dl: !ref train_dl
  test_dl: !ref test_dl
  epochs: 10
  eval_freq: 1000
  gradient_clip: 1.0
  use_amp: true  # Enable mixed precision training
